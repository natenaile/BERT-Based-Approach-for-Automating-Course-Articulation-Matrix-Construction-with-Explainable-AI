{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nlpaug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VeFJn9hOuqX",
        "outputId": "f611b55a-5aeb-4587-86c9-90d6186314fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.10/dist-packages (1.1.11)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.32.3)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.16.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "data = pd.read_csv('/content/copopsofinaldataset.csv')\n",
        "data = data[['CO Description', 'PO/PSO Description', 'Score (0-3)']]"
      ],
      "metadata": {
        "id": "r7B8bqTKMYvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRSjOtXAKlA_",
        "outputId": "9c2c71ca-6ce7-43a1-c76c-b47733cbafb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score (0-3)\n",
            "0    1134\n",
            "3     580\n",
            "2      84\n",
            "1      42\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "score_counts = data['Score (0-3)'].value_counts()\n",
        "print(score_counts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Augmentation\n",
        "import pandas as pd\n",
        "import nlpaug.augmenter.word as naw\n",
        "import random\n",
        "\n",
        "data = pd.read_csv('/content/copopsofinaldataset.csv')\n",
        "\n",
        "aug = naw.SynonymAug(aug_p=0.3)\n",
        "\n",
        "def augment_text_with_synonyms(text, n=1):\n",
        "    augmented_texts = []\n",
        "    for _ in range(n):\n",
        "        augmented_text = aug.augment(text)\n",
        "        augmented_texts.append(augmented_text[0])\n",
        "    return augmented_texts\n",
        "\n",
        "balanced_data = pd.DataFrame()\n",
        "\n",
        "target_samples = 2500\n",
        "\n",
        "for score in range(4):\n",
        "    class_data = data[data['Score (0-3)'] == score]\n",
        "    num_needed = target_samples - len(class_data)\n",
        "\n",
        "    augmented_samples = []\n",
        "    if num_needed > 0:\n",
        "        for _, row in class_data.iterrows():\n",
        "            samples_per_row = max(1, num_needed // len(class_data))\n",
        "            augmented_texts = augment_text_with_synonyms(row['CO Description'], n=samples_per_row)\n",
        "\n",
        "            for text in augmented_texts:\n",
        "                new_row = row.copy()\n",
        "                new_row['CO Description'] = text\n",
        "                augmented_samples.append(new_row)\n",
        "\n",
        "            num_needed -= len(augmented_texts)\n",
        "            if num_needed <= 0:\n",
        "                break\n",
        "\n",
        "    if num_needed > 0:\n",
        "        for _, row in class_data.iterrows():\n",
        "            samples_per_row = max(1, num_needed // len(class_data))\n",
        "            augmented_texts = augment_text_with_synonyms(row['PO/PSO Description'], n=samples_per_row)\n",
        "\n",
        "            for text in augmented_texts:\n",
        "                new_row = row.copy()\n",
        "                new_row['PO/PSO Description'] = text\n",
        "                augmented_samples.append(new_row)\n",
        "\n",
        "            num_needed -= len(augmented_texts)\n",
        "            if num_needed <= 0:\n",
        "                break\n",
        "\n",
        "    augmented_df = pd.DataFrame(augmented_samples[:target_samples - len(class_data)])\n",
        "    balanced_class_data = pd.concat([class_data, augmented_df])\n",
        "\n",
        "    balanced_data = pd.concat([balanced_data, balanced_class_data])\n",
        "\n",
        "balanced_data = balanced_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print(balanced_data['Score (0-3)'].value_counts())\n",
        "\n",
        "balanced_data.to_csv('/content/balanced_copopsofinaldataset_new1.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkI2plb9lx1a",
        "outputId": "0119b66b-e9fe-4583-dc36-ac6a5d63d722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score (0-3)\n",
            "0    2500\n",
            "1    2158\n",
            "3    2151\n",
            "2    2140\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYH28daCUKpI",
        "outputId": "6422174e-3b03-4435-811a-5fd2ec163c4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Augmentation (Without Shuffling).\n",
        "import pandas as pd\n",
        "import nlpaug.augmenter.word as naw\n",
        "import random\n",
        "\n",
        "data = pd.read_csv('/content/copopsofinaldataset.csv')\n",
        "\n",
        "aug = naw.SynonymAug(aug_p=0.3)\n",
        "\n",
        "def augment_text_with_synonyms(text, n=1):\n",
        "    augmented_texts = []\n",
        "    for _ in range(n):\n",
        "        augmented_text = aug.augment(text)\n",
        "        augmented_texts.append(augmented_text[0])\n",
        "    return augmented_texts\n",
        "\n",
        "balanced_data = pd.DataFrame()\n",
        "\n",
        "target_samples = 2500\n",
        "\n",
        "for score in range(4):\n",
        "    class_data = data[data['Score (0-3)'] == score]\n",
        "    num_needed = target_samples - len(class_data)\n",
        "\n",
        "    augmented_samples = []\n",
        "    if num_needed > 0:\n",
        "        for _, row in class_data.iterrows():\n",
        "            samples_per_row = max(1, num_needed // len(class_data))\n",
        "            augmented_texts = augment_text_with_synonyms(row['CO Description'], n=samples_per_row)\n",
        "\n",
        "            for text in augmented_texts:\n",
        "                new_row = row.copy()\n",
        "                new_row['CO Description'] = text\n",
        "                augmented_samples.append(new_row)\n",
        "\n",
        "            num_needed -= len(augmented_texts)\n",
        "            if num_needed <= 0:\n",
        "                break\n",
        "\n",
        "    if num_needed > 0:\n",
        "        for _, row in class_data.iterrows():\n",
        "            samples_per_row = max(1, num_needed // len(class_data))\n",
        "            augmented_texts = augment_text_with_synonyms(row['PO/PSO Description'], n=samples_per_row)\n",
        "\n",
        "            for text in augmented_texts:\n",
        "                new_row = row.copy()\n",
        "                new_row['PO/PSO Description'] = text\n",
        "                augmented_samples.append(new_row)\n",
        "\n",
        "            num_needed -= len(augmented_texts)\n",
        "            if num_needed <= 0:\n",
        "                break\n",
        "\n",
        "    augmented_df = pd.DataFrame(augmented_samples[:target_samples - len(class_data)])\n",
        "    balanced_class_data = pd.concat([class_data, augmented_df])\n",
        "\n",
        "    balanced_data = pd.concat([balanced_data, balanced_class_data])\n",
        "\n",
        "# No shuffling, keeping the original order intact\n",
        "\n",
        "print(balanced_data['Score (0-3)'].value_counts())\n",
        "\n",
        "balanced_data.to_csv('/content/balanced_copopsofinaldataset_new1_withoutshuffling.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp8vz3lgT11o",
        "outputId": "ada93aa2-3139-4d82-bcc9-9a7b3ec87f6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score (0-3)\n",
            "0    2500\n",
            "1    2158\n",
            "3    2151\n",
            "2    2140\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}